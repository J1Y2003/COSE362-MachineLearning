{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Ogl4M8bBZMhU","executionInfo":{"status":"ok","timestamp":1727975992574,"user_tz":-540,"elapsed":20908,"user":{"displayName":"Jaewon Youm","userId":"13291960395408720790"}},"outputId":"66dc20af-fbc8-4c21-b7c8-d7b6a6accf22","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["### Importing relevent libraries and reading data files"],"metadata":{"id":"TqcqC22O0kIn"}},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":true,"id":"2zZCGwmKZ1yR","executionInfo":{"status":"ok","timestamp":1727975995079,"user_tz":-540,"elapsed":2507,"user":{"displayName":"Jaewon Youm","userId":"13291960395408720790"}}},"outputs":[],"source":["import numpy as np\n","import scipy as sp\n","\n","training_data_and_class = np.loadtxt(\"/content/drive/MyDrive/2학년 2학기/COSE362-MachineLearning/Hw1/train.txt\") #(60290, 14(13 feature + 1 class))\n","#training_data_and_class = np.loadtxt(\"/content/drive/MyDrive/2학년 2학기/COSE362-MachineLearning/Hw1/personal_test.txt\")\n","true_class = training_data_and_class[:, -1] #(60290,)\n","train_data_0 = training_data_and_class[:, :-1][true_class == 0]\n","train_data_1 = training_data_and_class[:, :-1][true_class == 1]"]},{"cell_type":"markdown","source":["### Self-defined functions"],"metadata":{"id":"MhvZRU9MmM1I"}},{"cell_type":"code","source":["def compute_responsibilities_vectorized(theta, data, K):\n","    \"\"\"Vectorized computation of responsibilities.\"\"\"\n","    component_dist, means, covariances = theta\n","    data_size, feat_dim = data.shape\n","\n","    # Precompute the multivariate normal densities for all components and all data points\n","    prob_matrix = np.zeros((data_size, K))\n","    for k in range(K):\n","        prob_matrix[:, k] = sp.stats.multivariate_normal.pdf(data, means[k], covariances[k], allow_singular=True) * component_dist[k]\n","\n","    # Calculate responsibilities by normalizing over all components\n","    responsibility_matrix = prob_matrix / prob_matrix.sum(axis=1, keepdims=True)\n","    return responsibility_matrix"],"metadata":{"id":"XHjxYNHvi7D2","executionInfo":{"status":"ok","timestamp":1727975995079,"user_tz":-540,"elapsed":3,"user":{"displayName":"Jaewon Youm","userId":"13291960395408720790"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["### GMM model"],"metadata":{"id":"TcW1NtlZPyo_"}},{"cell_type":"code","source":["def GMM_training(data, K):\n","  it = 0\n","  data_size, feat_dim = np.shape(data)\n","\n","  #initialization\n","\n","  component_dist = np.zeros((K,))\n","  mean_k = np.zeros((K, feat_dim))\n","  cov_k = np.zeros((K, feat_dim, feat_dim))\n","  component_dist[:] = 1/K #(K,)\n","  mean_k = data[np.random.choice(data.shape[0], size=K, replace=False)]\n","  cov_init = np.cov(data, rowvar=False)\n","  cov_k[:] = cov_init #(K, feat_dim, feat_dim)\n","  theta = [component_dist, mean_k, cov_k]\n","  convergence = 0\n","\n","  #iteration until theta convergence\n","  while True:\n","    it += 1\n","    #Expectation\n","    responsibility = compute_responsibilities_vectorized(theta, data, K)\n","    #Mazimization\n","    new_component_dist = np.zeros(np.shape(component_dist))\n","    new_mean_k = np.zeros(np.shape(mean_k))\n","    new_cov_k = np.zeros(np.shape(cov_k))\n","\n","    for k in range(K):\n","      new_mean_k_num = np.zeros((feat_dim,))\n","      new_cov_k_num = np.zeros((feat_dim, feat_dim))\n","      new_mean_k_denum = 0\n","      for t in range(data_size):\n","        new_component_dist[k] += responsibility[t][k]\n","        new_mean_k_num += responsibility[t][k] * data[t]\n","        new_cov_k_num += responsibility[t][k] * np.outer(data[t], data[t])\n","        new_mean_k_denum += responsibility[t][k]\n","\n","      new_component_dist[k] /= data_size\n","      new_mean_k[k] = new_mean_k_num / new_mean_k_denum\n","      new_cov_k[k] = (new_cov_k_num / new_mean_k_denum) - np.outer(new_mean_k[k], new_mean_k[k])\n","\n","\n","    new_theta = [new_component_dist, new_mean_k, new_cov_k]\n","    #check convergence\n","    \"\"\"\n","    below method was discarded as it was too computationally expensive\n","    log_likelihood = 0\n","    log_likelihood_new = 0\n","    for t in range(data_size):\n","      likelihood_inner = 0\n","      likelihood_inner_new = 0\n","      for k in range(K):\n","        likelihood_inner += sp.stats.multivariate_normal.pdf(data[t], mean_k[k], cov_k[k]) * component_dist[k]\n","        likelihood_inner_new += sp.stats.multivariate_normal.pdf(data[t], new_mean_k[k], new_cov_k[k]) * new_component_dist[k]\n","      log_likelihood += np.log(likelihood_inner)\n","      log_likelihood_new += np.log(likelihood_inner_new)\n","    print(\"old, new likelihood calculated\")\n","\n","    if (abs(log_likelihood_new - log_likelihood) < 10 ** -4):\n","      break\n","    else:\n","      theta = new_theta\n","    \"\"\"\n","    old_component_dist, old_means, old_covariances = theta\n","    new_component_dist, new_means, new_covariances = new_theta\n","\n","    # Check for changes in means, covariances, and component distributions\n","    mean_ratio = np.mean(abs(new_means / old_means))\n","    cov_ratio = np.mean([np.mean(abs(new_covariances[i] / old_covariances[i])) for i in range(len(old_covariances))])\n","    comp_dist_ratio = np.mean(abs(new_component_dist / old_component_dist))\n","    total_diff = abs((sum([mean_ratio, cov_ratio, comp_dist_ratio]) / 3) - 1)\n","    print(\"total_diff:\", total_diff)\n","    if (convergence > 3):\n","      break\n","    if (total_diff < 10 ** -2):\n","      convergence += 1\n","    else:\n","      convergence = 0\n","    theta = new_theta\n","\n","  return new_theta, it"],"metadata":{"id":"OC3zVPwpAaK7","executionInfo":{"status":"ok","timestamp":1727975995079,"user_tz":-540,"elapsed":2,"user":{"displayName":"Jaewon Youm","userId":"13291960395408720790"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### Running"],"metadata":{"id":"yKlSlJvif7Fy"}},{"cell_type":"code","source":["\n","\n","for K in range(1, 11):\n","  GMM_0_theta, iterations = GMM_training(train_data_0, K)\n","  print(\"K:\", K, \"iterations:\", iterations - 4)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BCAAyXq9f6Sw","outputId":"3079f579-71f0-4931-ffb6-3a8f98f2d78e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total_diff: 0.06868553231871444\n","total_diff: 0.0\n","total_diff: 0.0\n"]}]}]}