{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Importing relevent libraries and reading data files"
      ],
      "metadata": {
        "id": "TqcqC22O0kIn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "collapsed": true,
        "id": "2zZCGwmKZ1yR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "\n",
        "training_data_and_class = np.loadtxt(\"/content/drive/MyDrive/2학년 2학기/COSE362-MachineLearning/Hw1/train.txt\") #(60290, 14(13 feature + 1 class))\n",
        "#training_data_and_class = np.loadtxt(\"/content/drive/MyDrive/2학년 2학기/COSE362-MachineLearning/Hw1/personal_test.txt\")\n",
        "true_class = training_data_and_class[:, -1] #(60290,)\n",
        "train_data_0 = training_data_and_class[:, :-1][true_class == 0]\n",
        "train_data_1 = training_data_and_class[:, :-1][true_class == 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self-defined functions"
      ],
      "metadata": {
        "id": "MhvZRU9MmM1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_responsibilities_vectorized(theta, data, K):\n",
        "    \"\"\"Vectorized computation of responsibilities.\"\"\"\n",
        "    component_dist, means, covariances = theta\n",
        "    data_size, feat_dim = data.shape\n",
        "\n",
        "    # Precompute the multivariate normal densities for all components and all data points\n",
        "    prob_matrix = np.zeros((data_size, K))\n",
        "    for k in range(K):\n",
        "        prob_matrix[:, k] = sp.stats.multivariate_normal.pdf(data, means[k], covariances[k], allow_singular=True) * component_dist[k]\n",
        "\n",
        "    # Calculate responsibilities by normalizing over all components\n",
        "    responsibility_matrix = prob_matrix / prob_matrix.sum(axis=1, keepdims=True)\n",
        "    return responsibility_matrix"
      ],
      "metadata": {
        "id": "XHjxYNHvi7D2"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GMM model"
      ],
      "metadata": {
        "id": "TcW1NtlZPyo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GMM_training(data, K):\n",
        "  it = 0\n",
        "  data_size, feat_dim = np.shape(data)\n",
        "\n",
        "  #initialization\n",
        "\n",
        "  component_dist = np.zeros((K,))\n",
        "  mean_k = np.zeros((K, feat_dim))\n",
        "  cov_k = np.zeros((K, feat_dim, feat_dim))\n",
        "  component_dist[:] = 1/K #(K,)\n",
        "  mean_k = data[np.random.choice(data.shape[0], size=K, replace=False)]\n",
        "  cov_init = np.cov(data, rowvar=False)\n",
        "  cov_k[:] = cov_init #(K, feat_dim, feat_dim)\n",
        "  theta = [component_dist, mean_k, cov_k]\n",
        "  convergence = 0\n",
        "\n",
        "  #iteration until theta convergence\n",
        "  while True:\n",
        "    it += 1\n",
        "    #Expectation\n",
        "    responsibility = compute_responsibilities_vectorized(theta, data, K)\n",
        "    #Mazimization\n",
        "    new_component_dist = np.zeros(np.shape(component_dist))\n",
        "    new_mean_k = np.zeros(np.shape(mean_k))\n",
        "    new_cov_k = np.zeros(np.shape(cov_k))\n",
        "\n",
        "    for k in range(K):\n",
        "      new_mean_k_num = np.zeros((feat_dim,))\n",
        "      new_cov_k_num = np.zeros((feat_dim, feat_dim))\n",
        "      new_mean_k_denum = 0\n",
        "      for t in range(data_size):\n",
        "        new_component_dist[k] += responsibility[t][k]\n",
        "        new_mean_k_num += responsibility[t][k] * data[t]\n",
        "        new_cov_k_num += responsibility[t][k] * np.outer(data[t], data[t])\n",
        "        new_mean_k_denum += responsibility[t][k]\n",
        "\n",
        "      new_component_dist[k] /= data_size\n",
        "      new_mean_k[k] = new_mean_k_num / new_mean_k_denum\n",
        "      new_cov_k[k] = (new_cov_k_num / new_mean_k_denum) - np.outer(new_mean_k[k], new_mean_k[k])\n",
        "\n",
        "\n",
        "    new_theta = [new_component_dist, new_mean_k, new_cov_k]\n",
        "    #check convergence\n",
        "    \"\"\"\n",
        "    below method was discarded as it was too computationally expensive\n",
        "    log_likelihood = 0\n",
        "    log_likelihood_new = 0\n",
        "    for t in range(data_size):\n",
        "      likelihood_inner = 0\n",
        "      likelihood_inner_new = 0\n",
        "      for k in range(K):\n",
        "        likelihood_inner += sp.stats.multivariate_normal.pdf(data[t], mean_k[k], cov_k[k]) * component_dist[k]\n",
        "        likelihood_inner_new += sp.stats.multivariate_normal.pdf(data[t], new_mean_k[k], new_cov_k[k]) * new_component_dist[k]\n",
        "      log_likelihood += np.log(likelihood_inner)\n",
        "      log_likelihood_new += np.log(likelihood_inner_new)\n",
        "    print(\"old, new likelihood calculated\")\n",
        "\n",
        "    if (abs(log_likelihood_new - log_likelihood) < 10 ** -4):\n",
        "      break\n",
        "    else:\n",
        "      theta = new_theta\n",
        "    \"\"\"\n",
        "    old_component_dist, old_means, old_covariances = theta\n",
        "    new_component_dist, new_means, new_covariances = new_theta\n",
        "\n",
        "    # Check for changes in means, covariances, and component distributions\n",
        "    mean_ratio = np.mean(abs(new_means / old_means))\n",
        "    cov_ratio = np.mean([np.mean(abs(new_covariances[i] / old_covariances[i])) for i in range(len(old_covariances))])\n",
        "    comp_dist_ratio = np.mean(abs(new_component_dist / old_component_dist))\n",
        "    total_diff = abs((sum([mean_ratio, cov_ratio, comp_dist_ratio]) / 3) - 1)\n",
        "    print(\"total_diff:\", total_diff)\n",
        "    if (convergence > 3):\n",
        "      break\n",
        "    if (total_diff < 10 ** -2):\n",
        "      convergence += 1\n",
        "    else:\n",
        "      convergence = 0\n",
        "    theta = new_theta\n",
        "\n",
        "  return new_theta, it"
      ],
      "metadata": {
        "id": "OC3zVPwpAaK7"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running"
      ],
      "metadata": {
        "id": "yKlSlJvif7Fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for K in range(1, 11):\n",
        "  GMM_0_theta, iterations = GMM_training(train_data_0, K)\n",
        "  print(\"K:\", K, \"iterations:\", iterations - 4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCAAyXq9f6Sw",
        "outputId": "f18aea08-2161-4358-8cd6-7c59d332b1f1"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_diff: 0.2117121024245785\n",
            "total_diff: 0.0\n",
            "total_diff: 0.0\n",
            "total_diff: 0.0\n",
            "total_diff: 0.0\n",
            "total_diff: 0.0\n",
            "K: 1 iterations: 2\n",
            "total_diff: 0.7091523038679497\n",
            "total_diff: 0.21064059968997872\n",
            "total_diff: 0.03647987208744241\n",
            "total_diff: 0.09582272621454524\n",
            "total_diff: 0.021360363925228443\n",
            "total_diff: 0.016235087722640706\n",
            "total_diff: 0.004215144449933428\n",
            "total_diff: 0.02856763572777865\n",
            "total_diff: 0.011570168665048008\n",
            "total_diff: 0.00664629445498921\n",
            "total_diff: 0.0015624570348746758\n",
            "total_diff: 0.0370304347101893\n",
            "total_diff: 0.011251652010584312\n",
            "total_diff: 0.002025925270128237\n",
            "total_diff: 0.011484373022278005\n",
            "total_diff: 0.01244671843240397\n",
            "total_diff: 0.029615168917768386\n",
            "total_diff: 0.012805134666196816\n",
            "total_diff: 0.0031207531526196153\n",
            "total_diff: 0.00031312106352643365\n",
            "total_diff: 0.0012244921822679444\n",
            "total_diff: 0.0007632051674495433\n",
            "total_diff: 0.004190271451908312\n",
            "K: 2 iterations: 19\n",
            "total_diff: 1.001786196053668\n",
            "total_diff: 0.06751763187815363\n",
            "total_diff: 0.04726018933530818\n",
            "total_diff: 0.032125356135431105\n",
            "total_diff: 0.02528085491281984\n",
            "total_diff: 0.00845037127741155\n",
            "total_diff: 0.02392966901436755\n",
            "total_diff: 0.004366902179417709\n",
            "total_diff: 0.02136961944685667\n",
            "total_diff: 0.02946497597069042\n",
            "total_diff: 0.015189481366775448\n",
            "total_diff: 0.0334793555761026\n",
            "total_diff: 0.007641571001072878\n",
            "total_diff: 0.007881511199752111\n",
            "total_diff: 0.028886505557909947\n",
            "total_diff: 0.011077991358807937\n",
            "total_diff: 0.0308620636684378\n",
            "total_diff: 0.009582068114398767\n",
            "total_diff: 0.005445812106222858\n",
            "total_diff: 0.0024470535336102994\n",
            "total_diff: 0.04652850493187155\n",
            "total_diff: 0.007130785483822066\n",
            "total_diff: 0.009918897172030094\n",
            "total_diff: 0.007318706336625125\n",
            "total_diff: 0.004363151129507692\n",
            "total_diff: 0.002966527644493011\n",
            "K: 3 iterations: 22\n",
            "total_diff: 0.7801921883678811\n",
            "total_diff: 0.042473713034256066\n",
            "total_diff: 2.629758637856286\n",
            "total_diff: 0.08435577203455269\n",
            "total_diff: 0.06950389011909852\n",
            "total_diff: 0.09040823798311615\n",
            "total_diff: 0.06497846003339958\n",
            "total_diff: 0.020997620865504052\n",
            "total_diff: 0.3341798273964658\n",
            "total_diff: 0.3034001326753104\n",
            "total_diff: 0.00936492418892354\n",
            "total_diff: 0.024642872606178257\n",
            "total_diff: 0.2039412137258807\n",
            "total_diff: 0.02827109381731696\n",
            "total_diff: 0.05540958324513823\n",
            "total_diff: 0.008983908691927445\n",
            "total_diff: 0.12204722190245731\n",
            "total_diff: 0.02533258500509228\n",
            "total_diff: 0.03410351973188175\n",
            "total_diff: 0.014887752312350955\n",
            "total_diff: 0.014511037977104957\n",
            "total_diff: 0.088341512184547\n",
            "total_diff: 0.013419547394282416\n",
            "total_diff: 0.009187745156974847\n",
            "total_diff: 0.02033752041421555\n",
            "total_diff: 0.007468620596272402\n",
            "total_diff: 0.004788995533751006\n",
            "total_diff: 0.01096408681569172\n",
            "total_diff: 0.00833143735156927\n",
            "total_diff: 0.8507762088008735\n",
            "total_diff: 0.009385669477200098\n",
            "total_diff: 0.010543276594665807\n",
            "total_diff: 0.008608557914112192\n",
            "total_diff: 0.00628623546153162\n",
            "total_diff: 0.00551879177624337\n",
            "total_diff: 0.0005299785456978068\n",
            "total_diff: 0.0043738817903598015\n",
            "K: 4 iterations: 33\n",
            "total_diff: 0.664540310113529\n",
            "total_diff: 0.04014922132568022\n",
            "total_diff: 0.05559240188276182\n",
            "total_diff: 0.35406747282063744\n",
            "total_diff: 0.044950730617732804\n",
            "total_diff: 0.023670603342116348\n",
            "total_diff: 0.01659548961256685\n",
            "total_diff: 0.018346978666542668\n",
            "total_diff: 0.024364937579874457\n",
            "total_diff: 0.020340480323337573\n",
            "total_diff: 0.010925648969905222\n",
            "total_diff: 0.007479142810736139\n",
            "total_diff: 0.011745634760509382\n",
            "total_diff: 0.025857121966549768\n",
            "total_diff: 0.009762969434705004\n",
            "total_diff: 0.035438296422112714\n",
            "total_diff: 0.009359906236497073\n",
            "total_diff: 0.010993056710049798\n",
            "total_diff: 0.011251413887782613\n",
            "total_diff: 0.027270492851544814\n",
            "total_diff: 0.009703920730089743\n",
            "total_diff: 0.004707710308628377\n",
            "total_diff: 0.0035685533857658935\n",
            "total_diff: 0.006259376246040338\n",
            "total_diff: 0.017488075986779483\n",
            "K: 5 iterations: 21\n",
            "total_diff: 0.3493450555709723\n",
            "total_diff: 0.2314543282414585\n",
            "total_diff: 0.9348216834767225\n",
            "total_diff: 0.1608736750550257\n",
            "total_diff: 0.02109126725129773\n",
            "total_diff: 0.05171016362528813\n",
            "total_diff: 0.0496329695358404\n",
            "total_diff: 0.039009449766927995\n",
            "total_diff: 0.04239489770174121\n",
            "total_diff: 0.055298495420706306\n",
            "total_diff: 0.02248553013224419\n",
            "total_diff: 0.034552823978194036\n",
            "total_diff: 0.0006937577949892582\n",
            "total_diff: 0.035789155458535005\n",
            "total_diff: 0.010840519224611045\n",
            "total_diff: 0.33917740943389174\n",
            "total_diff: 0.014823483145310457\n",
            "total_diff: 0.016799931406754842\n",
            "total_diff: 0.03103402086086704\n",
            "total_diff: 0.003157399239273495\n",
            "total_diff: 0.002171840987267304\n",
            "total_diff: 0.01959303601033091\n",
            "total_diff: 0.013956346416834853\n",
            "total_diff: 0.006784703745673859\n",
            "total_diff: 0.005525637833809993\n",
            "total_diff: 0.0034037841979460692\n",
            "total_diff: 0.0608248662331452\n",
            "total_diff: 0.003659257895970769\n",
            "total_diff: 0.0019102205623162671\n",
            "total_diff: 0.0007675734641343634\n",
            "total_diff: 0.0018523394383183067\n",
            "total_diff: 0.002922658692860125\n",
            "K: 6 iterations: 28\n",
            "total_diff: 0.46361812241448885\n",
            "total_diff: 1.123345574251513\n",
            "total_diff: 0.07515886774503233\n",
            "total_diff: 0.30601162409416016\n",
            "total_diff: 0.02959304064598478\n",
            "total_diff: 0.039412729694014104\n",
            "total_diff: 0.10777077753420805\n",
            "total_diff: 0.003918916576781228\n",
            "total_diff: 0.057816054796897554\n",
            "total_diff: 0.029076238637109952\n",
            "total_diff: 0.045128489375819925\n",
            "total_diff: 0.015156448055504868\n",
            "total_diff: 0.02162767783479036\n",
            "total_diff: 0.004978944757233039\n",
            "total_diff: 0.04040585415590758\n",
            "total_diff: 0.016967814147769378\n",
            "total_diff: 7.05887121983384e-05\n",
            "total_diff: 0.0033552121541114976\n",
            "total_diff: 0.0021272501669608035\n",
            "total_diff: 0.003519793889395162\n",
            "total_diff: 0.007156284046958339\n",
            "K: 7 iterations: 17\n",
            "total_diff: 0.979951343764851\n",
            "total_diff: 0.13461266202158728\n",
            "total_diff: 0.07523736251339286\n",
            "total_diff: 0.013858126453304864\n",
            "total_diff: 0.07050285513963872\n",
            "total_diff: 0.40618286404602877\n",
            "total_diff: 0.015512987657165356\n",
            "total_diff: 0.1148681752515992\n",
            "total_diff: 0.07964624811231635\n",
            "total_diff: 0.1236355806471856\n",
            "total_diff: 0.45584099156269886\n",
            "total_diff: 0.03443096160847148\n",
            "total_diff: 0.05398303389353809\n",
            "total_diff: 0.002170184256311103\n",
            "total_diff: 0.05384468147046051\n",
            "total_diff: 0.01031191644223628\n",
            "total_diff: 0.01866548401756085\n",
            "total_diff: 0.013067933184685687\n",
            "total_diff: 0.0033968382047933066\n",
            "total_diff: 0.0012857452500264488\n",
            "total_diff: 2.8033612360145455e-05\n",
            "total_diff: 0.035205494935088844\n",
            "total_diff: 0.007598030056114702\n",
            "total_diff: 0.006168261316335633\n",
            "total_diff: 0.0011782341324362289\n",
            "total_diff: 0.0050045720323774745\n",
            "total_diff: 0.005226122628603891\n",
            "K: 8 iterations: 23\n",
            "total_diff: 0.7650871256567138\n",
            "total_diff: 0.26131907803243926\n",
            "total_diff: 0.12916426952778948\n",
            "total_diff: 0.04404193245510046\n",
            "total_diff: 0.018690433123916828\n",
            "total_diff: 0.00930435227306936\n",
            "total_diff: 0.027931025096969764\n",
            "total_diff: 0.013382673667734268\n",
            "total_diff: 0.009444381417309744\n",
            "total_diff: 0.6977168714385578\n",
            "total_diff: 0.026588973499299007\n",
            "total_diff: 0.01259025949133541\n",
            "total_diff: 0.036791289119896264\n",
            "total_diff: 0.0064100027918312374\n",
            "total_diff: 0.0075016620484833485\n",
            "total_diff: 0.010759521196209043\n",
            "total_diff: 0.012573958366476568\n",
            "total_diff: 0.013590953730157285\n",
            "total_diff: 0.0164189040499525\n",
            "total_diff: 0.13460056783946617\n",
            "total_diff: 0.036542453456822166\n",
            "total_diff: 0.0442606572406834\n",
            "total_diff: 0.007446665900078253\n",
            "total_diff: 0.005400232712831299\n",
            "total_diff: 0.003262492401412187\n",
            "total_diff: 0.008038686116539573\n",
            "total_diff: 0.00690394143567552\n",
            "K: 9 iterations: 23\n",
            "total_diff: 0.7342562222374609\n",
            "total_diff: 0.18071799549478262\n",
            "total_diff: 0.39050259828859524\n",
            "total_diff: 0.09671202900085163\n",
            "total_diff: 0.05765514353302903\n",
            "total_diff: 0.05132245198692087\n",
            "total_diff: 0.04347283308343264\n",
            "total_diff: 0.018221947750722167\n",
            "total_diff: 0.0873059438774395\n",
            "total_diff: 0.009765310086350176\n",
            "total_diff: 0.029023673048673926\n",
            "total_diff: 0.042560776024165525\n",
            "total_diff: 0.0446641811167241\n",
            "total_diff: 0.011365097132482793\n",
            "total_diff: 0.016778320267796465\n",
            "total_diff: 0.00896556418519201\n",
            "total_diff: 1.5477929457219375e-05\n",
            "total_diff: 0.002086603286367339\n",
            "total_diff: 0.004929121186427632\n",
            "total_diff: 0.01605231986539768\n",
            "K: 10 iterations: 16\n"
          ]
        }
      ]
    }
  ]
}